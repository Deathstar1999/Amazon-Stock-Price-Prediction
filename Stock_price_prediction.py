# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb

**Objective**

The task is to predict the day price direction of Amazon.com, Inc. (AMZN).

The stock market is very complex and highly volatile. In order to be profitable, we do not need to predict the correct price, but rather, the price direction: whether it will be higher or lower than the price that is today. If we predict it to be higher, we might as well buy some stocks, else, we should probably sell. Therefore, the target would be a binary classification whether the next day closing price will be higher than the opening price.

We have data for the period from 1997 up to the year 2020 that we have split into training (1997-2016), validation (2016-2018) and testing (2018-2020) periods. The data is available in the AMZN_train.csv, AMZN_val.csv and AMZN_test.csv files, respectively.

**Data Exploration**

In our initial exploration, we will load the data sets and see what data attributes are available to us. We will also plot the variables, to see if we can find some trends in the data, and explore the possibility of engineering some additional features. We will do the data loading and analysis in pandas, so let us load that library and begin exploring.
"""

import pandas as pd

# load the training set
df_train = pd.read_csv("AMZN_train.csv")

# (#rows, #columns)
df_train.shape

df_train.info()

df_train.describe()

# the above call to describe() works only for numerical columns
# 'Date' is an object and we need to call it separately
df_train["Date"].describe()

# print the time range
df_train["Date"].min(), df_train["Date"].max()

from matplotlib import pyplot as plt

plt.rcParams["figure.figsize"] = (12, 9)

_ = df_train.plot(x="Date", y=["Close", "Open", "High", "Low"])

def analyse(dataframe):
    """Runs an exploration analysis of the dataframe."""
    print("Shape", dataframe.shape, "\n")
    print("Columns", dataframe.columns, "\n")
    dataframe.info()
    print("\n", dataframe.describe(), "\n")
    print("The data ranges from", dataframe["Date"].min(), "to", dataframe["Date"].max())
    dataframe.plot(x="Date", y=["Close", "Open", "High", "Low"])

# read validation and test sets and then analyse them
df_val = pd.read_csv("AMZN_val.csv")
analyse(df_val)

df_test = pd.read_csv("AMZN_test.csv")
analyse(df_test)

# make sure that our data is sorted by date
df_train.sort_values(by="Date", inplace=True)
df_val.sort_values(by="Date", inplace=True)
df_test.sort_values(by="Date", inplace=True)

# notice that we shift by a period of '-1', this takes the next day's price direction for the current day
# a positive period will take the days from the past
df_train["Target"] = (df_train["Close"] > df_train["Open"]).shift(periods=-1, fill_value=0).astype(int)

df_train

df_train["Target"].value_counts()

df_val["Target"] = (df_val["Close"] > df_val["Open"]).shift(periods=-1, fill_value=0).astype(int)
df_val["Target"].value_counts()

df_test["Target"] = (df_test["Close"] > df_test["Open"]).shift(periods=-1, fill_value=0).astype(int)
df_test["Target"].value_counts()

"""**Feature Engineering**


We know that the stock prices are time-dependent and that the next day's price depends on prices (and many other things) from previous days.

We want to somehow take into account all the values in the last n days, capturing the trend, or the magnitude of price change.

A simple solution would be to calculate a moving average. With Pandas, we can use the rolling method to calculate moving averages. It provides us with an interface for sliding (in Pandas terminology - rolling) window calculations. The following cells calculate the 3- and 7-days moving average, and add them as a feature into the data set.
"""

df_train["Moving_Average_3"] = (df_train["Close"] - df_train["Open"]).rolling(window=3, min_periods=1).mean()
df_val["Moving_Average_3"] = (df_val["Close"] - df_val["Open"]).rolling(window=3, min_periods=1).mean()
df_test["Moving_Average_3"] = (df_test["Close"] - df_test["Open"]).rolling(window=3, min_periods=1).mean()

df_train["Moving_Average_7"] = (df_train["Close"] - df_train["Open"]).rolling(window=7, min_periods=1).mean()
df_val["Moving_Average_7"] = (df_val["Close"] - df_val["Open"]).rolling(window=7, min_periods=1).mean()
df_test["Moving_Average_7"] = (df_test["Close"] - df_test["Open"]).rolling(window=7, min_periods=1).mean()

# current price direction
df_train["Today_Direction"] = df_train["Close"] - df_train["Open"]
df_val["Today_Direction"] = df_val["Close"] - df_val["Open"]
df_test["Today_Direction"] = df_test["Close"] - df_test["Open"]

# price range
df_train["Price_Range"] = df_train["High"] - df_train["Low"]
df_val["Price_Range"] = df_val["High"] - df_val["Low"]
df_test["Price_Range"] = df_test["High"] - df_test["Low"]

df_train.sample(10, random_state=42)

"""**Classical Machine Learning Algorithms**


The sklearn library is the most popular library in Python for implementing classical machine learning algorithms. We can use it to try and test a few of them. In the following cells we implement:

Logistic regression

Decision tree

Random forest

Gradient boosting ensemble

After fitting them to the training data, we are going to evaluate their performance on the validation set by estimating the AUC metric.
"""

# this is the target column that we aim to predict
y_col = "Target"
# these are the input features for the models
X_cols = [
    "Open",
    "Close",
    "High",
    "Low",
    "Volume",
    "Adj Close",
    "Today_Direction",
    "Price_Range",
    "Moving_Average_3",
    "Moving_Average_7"
]

X_train = df_train[X_cols]
y_train = df_train[y_col]

X_val = df_val[X_cols]
y_val = df_val[y_col]

X_test = df_val[X_cols]
y_test = df_val[y_col]

"""**Logistic Regression**

We start our modeling phase with a LogisticRegression model.
"""

# for reproducibility
RANDOM_SEED = 42

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import RocCurveDisplay

# Initialize the Logistic Regression model
lr = LogisticRegression()

# Fit the model to the training set
lr.fit(X_train, y_train)

# Plot the ROC curve for the validation set
RocCurveDisplay.from_estimator(lr, X_val, y_val)

"""**Decsion Tree**



"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import RocCurveDisplay

# Define a random seed for reproducibility
RANDOM_SEED = 42

# Initialize the Decision Tree Classifier
dt = DecisionTreeClassifier(random_state=RANDOM_SEED)

# Fit the model to the training data
dt.fit(X_train, y_train)

# Plot the ROC curve for the validation set
RocCurveDisplay.from_estimator(dt, X_val, y_val)

"""The decision tree outperforms the logistic regression model by 0.02, and its AUC is above 0.5!

**Random Forest**

Now we will try to use many decision trees, i.e., a forest. Random forest is an ensemble model that builds multiple decision trees, each with a different (random) sub-set of attributes.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import RocCurveDisplay

# Define a random seed for reproducibility
RANDOM_SEED = 42

# Initialize the Random Forest Classifier
rf = RandomForestClassifier(random_state=RANDOM_SEED)

# Fit the model to the training data
rf.fit(X_train, y_train)

# Plot the ROC curve for the validation set
RocCurveDisplay.from_estimator(rf, X_val, y_val)

"""Contrary to our expectations, the model does not outperform the decision tree, it actually performs the same.

**Gradient Boosting Ensemble**

One last ensemble technique that we would like to try is gradient boosting. This algorithm sets up the stage for our next part, where we will try a deep learning approach to solve the problem.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import RocCurveDisplay

# Define a random seed for reproducibility
RANDOM_SEED = 42

# Initialize the Gradient Boosting Classifier
gb = GradientBoostingClassifier(random_state=RANDOM_SEED)

# Fit the model to the training data
gb.fit(X_train, y_train)

# Plot the ROC curve for the validation set
RocCurveDisplay.from_estimator(gb, X_val, y_val)

"""Indeed it does, the gradient boosting classifier outperform previous models and scored 0.55 AUC!

We find out that gradient boosting works best for this data set. In the next section, we will train a deep learning model with the aim to outperform the baseline set here, i.e., AUC = 0.55.

**Conclusion**

The gradient boosting classifier provided the best AUC score on the validation set. It is a common machine learning practice to train multiple models on the same train/validation data set and provide a model that works best. To simulate a production environment, we have held the test set aside until now.

In the next cell, we are going to evaluate the performance of the gradient boosting classifier on the test set. This is simple as calling plot_roc_curve with the test set instead of the validation one.

As a bonus, in the last cell, we are showing a feature importance plot, which plots the importance of each feature in regard to the predictive performance of the model (the higher the value the more important the feature is for determining the value of the target variable).
"""

RocCurveDisplay.from_estimator(gb, X_test, y_test)

import numpy as np
# Calculate feature importances
importances = gb.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Rearrange feature names so they match the sorted feature importances
names = [df_train[X_cols].columns[i] for i in indices]

_ = plt.figure(figsize=(9, 7))
plt.bar(names, importances[indices])
_ = plt.title("Feature importance")
_ = plt.xticks(rotation=20, fontsize = 8)